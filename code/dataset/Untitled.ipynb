{
 "cells": [
  {
   "cell_type": "raw",
   "id": "32f1c265-e61e-4b4e-9b24-871464d4a8ca",
   "metadata": {},
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39ba5d3-3b9f-41dc-ab72-9006a6e666f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.98-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting comtypes (from pyttsx3)\n",
      "  Downloading comtypes-1.4.10-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pypiwin32 (from pyttsx3)\n",
      "  Downloading pypiwin32-223-py3-none-any.whl.metadata (236 bytes)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\shara\\anaconda3\\envs\\projects\\lib\\site-packages (from pyttsx3) (308)\n",
      "Downloading pyttsx3-2.98-py3-none-any.whl (34 kB)\n",
      "Downloading comtypes-1.4.10-py3-none-any.whl (241 kB)\n",
      "Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Installing collected packages: pypiwin32, comtypes, pyttsx3\n",
      "Successfully installed comtypes-1.4.10 pypiwin32-223 pyttsx3-2.98\n"
     ]
    }
   ],
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5811c990-7fba-410c-9979-b020aa727c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Using cached pyttsx3-2.98-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting comtypes (from pyttsx3)\n",
      "  Using cached comtypes-1.4.10-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pypiwin32 (from pyttsx3)\n",
      "  Using cached pypiwin32-223-py3-none-any.whl.metadata (236 bytes)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\shara\\anaconda3\\lib\\site-packages (from pyttsx3) (305.1)\n",
      "Using cached pyttsx3-2.98-py3-none-any.whl (34 kB)\n",
      "Using cached comtypes-1.4.10-py3-none-any.whl (241 kB)\n",
      "Using cached pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Installing collected packages: pypiwin32, comtypes, pyttsx3\n",
      "Successfully installed comtypes-1.4.10 pypiwin32-223 pyttsx3-2.98\n"
     ]
    }
   ],
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb55b73-7b3b-4e8a-bb48-3c11d0ca085d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 3, Smile: 0\n",
      "Person 1 - Eyes: 4, Smile: 0\n",
      "Person 1 - Eyes: 5, Smile: 0\n",
      "Person 1 - Eyes: 6, Smile: 0\n",
      "Person 1 - Eyes: 7, Smile: 1\n",
      "Person 1 - Eyes: 8, Smile: 2\n",
      "Person 1 - Eyes: 9, Smile: 3\n",
      "Person 1 - Eyes: 10, Smile: 4\n",
      "Person 1 - Eyes: 11, Smile: 5\n",
      "Person 1 - Eyes: 12, Smile: 6\n",
      "Person 1 - Eyes: 13, Smile: 5\n",
      "Person 1 - Eyes: 14, Smile: 6\n",
      "Person 1 - Eyes: 15, Smile: 7\n",
      "Person 1 - Eyes: 16, Smile: 8\n",
      "Person 1 - Eyes: 17, Smile: 9\n",
      "Person 1 - Eyes: 18, Smile: 10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pyttsx3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 108\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 89\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Check if all detected faces are smiling\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_faces \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m detected_faces \u001b[38;5;241m==\u001b[39m smiling_faces:  \n\u001b[1;32m---> 89\u001b[0m     speak(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3, 2, 1, Cheese!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Save the clean frame (without rectangles)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaptured_image_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m, in \u001b[0;36mspeak\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspeak\u001b[39m(text):\n\u001b[1;32m---> 12\u001b[0m     engine \u001b[38;5;241m=\u001b[39m pyttsx3\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m     13\u001b[0m     engine\u001b[38;5;241m.\u001b[39msay(text)\n\u001b[0;32m     14\u001b[0m     engine\u001b[38;5;241m.\u001b[39mrunAndWait()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyttsx3' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load Haar cascade classifiers for face, eyes, and smile detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "stable_eye_count = {}\n",
    "stable_smile_count = {}\n",
    "STABLE_THRESHOLD = 10  # Require detections over multiple frames\n",
    "\n",
    "def detect_faces(frame):\n",
    "    global stable_eye_count, stable_smile_count\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "    \n",
    "    detected_faces = 0\n",
    "    smiling_faces = 0\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Detect eyes and smiles\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=6)\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.5, minNeighbors=25)\n",
    "        \n",
    "        face_id = f\"face_{detected_faces}\"\n",
    "        detected_faces += 1\n",
    "        \n",
    "        # Initialize counts if not present\n",
    "        if face_id not in stable_eye_count:\n",
    "            stable_eye_count[face_id] = 0\n",
    "        if face_id not in stable_smile_count:\n",
    "            stable_smile_count[face_id] = 0\n",
    "        \n",
    "        # Update counts based on detection stability\n",
    "        stable_eye_count[face_id] = max(0, stable_eye_count[face_id] - 1) if len(eyes) < 2 else stable_eye_count[face_id] + 1\n",
    "        stable_smile_count[face_id] = max(0, stable_smile_count[face_id] - 1) if len(smiles) == 0 else stable_smile_count[face_id] + 1\n",
    "        \n",
    "        print(f\"Person {detected_faces} - Eyes: {stable_eye_count[face_id]}, Smile: {stable_smile_count[face_id]}\")\n",
    "        \n",
    "        # Determine colors for rectangles\n",
    "        face_color = (0, 255, 0) if stable_eye_count[face_id] >= STABLE_THRESHOLD and stable_smile_count[face_id] >= STABLE_THRESHOLD else (0, 0, 255)\n",
    "        smile_color = (0, 255, 0) if stable_smile_count[face_id] >= STABLE_THRESHOLD else (0, 0, 255)\n",
    "        \n",
    "        # Draw face rectangle\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), face_color, 2)  \n",
    "        \n",
    "        # Draw smile rectangle if detected\n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            cv2.rectangle(roi_color, (sx, sy), (sx+sw, sy+sh), smile_color, 2)  # Rectangle over the mouth\n",
    "        \n",
    "        if stable_eye_count[face_id] >= STABLE_THRESHOLD and stable_smile_count[face_id] >= STABLE_THRESHOLD:\n",
    "            smiling_faces += 1\n",
    "    \n",
    "    return frame, detected_faces, smiling_faces\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)  # Increase FPS for smoother capture\n",
    "    \n",
    "    pic_count = 1\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Make a copy of the original frame before drawing rectangles\n",
    "        clean_frame = frame.copy()\n",
    "        \n",
    "        frame, detected_faces, smiling_faces = detect_faces(frame)\n",
    "        \n",
    "        cv2.imshow('Face Detection', frame)\n",
    "        \n",
    "        # Check if all detected faces are smiling\n",
    "        if detected_faces > 0 and detected_faces == smiling_faces:  \n",
    "            speak(\"3, 2, 1, Cheese!\")\n",
    "            \n",
    "            # Save the clean frame (without rectangles)\n",
    "            filename = f\"captured_image_{pic_count}.jpg\"\n",
    "            cv2.imwrite(filename, clean_frame)\n",
    "            \n",
    "            print(f\"Console Output: Picture captured and saved as {filename}!\")\n",
    "            speak(\"Picture taken!\")\n",
    "            break  # Exit after taking the picture\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:  # Press 'q' or ESC to exit\n",
    "            print(\"Console Output: Exiting...\")\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f82ae60b-92e0-4735-966f-d1499cc80fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "\n",
    "# Load pre-trained face detection model  \n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')  \n",
    "\n",
    "# Open webcam  \n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "while True:  \n",
    "    ret, frame = cap.read()  \n",
    "    if not ret:  \n",
    "        break  \n",
    "\n",
    "    # Convert to grayscale for better accuracy  \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  \n",
    "\n",
    "    # Detect faces  \n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))  \n",
    "\n",
    "    for (x, y, w, h) in faces:  \n",
    "        # Draw bounding box around the face  \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)  \n",
    "\n",
    "        # Add label  \n",
    "        cv2.putText(frame, \"Face Detected\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)  \n",
    "\n",
    "    # Show output  \n",
    "    cv2.imshow(\"Face Detection\", frame)  \n",
    "\n",
    "    # Press 'q' to exit  \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  \n",
    "        break  \n",
    "\n",
    "cap.release()  \n",
    "cv2.destroyAllWindows()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645c2ce-bd32-4afa-a7fb-d61a463eafc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install ultralytics opencv-python numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d7b3f7-4e3e-4980-b6da-cb1a9f39c874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6.23M/6.23M [00:03<00:00, 1.96MB/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'captured_image_1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[0;32m      8\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaptured_image_1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Run YOLO detection\u001b[39;00m\n\u001b[0;32m     12\u001b[0m results \u001b[38;5;241m=\u001b[39m model(image)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\utils\\patches.py:25\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(filename, flags)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m, flags: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    Read an image from a file.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m        (np.ndarray): The read image.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mimdecode(np\u001b[38;5;241m.\u001b[39mfromfile(filename, np\u001b[38;5;241m.\u001b[39muint8), flags)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'captured_image_1.jpg'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load YOLO model (Ensure you have the correct YOLOv8 model)\n",
    "model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "# Load image\n",
    "image_path = \"captured_image_1.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Run YOLO detection\n",
    "results = model(image)\n",
    "\n",
    "# Process results and display output\n",
    "for result in results:\n",
    "    # Get annotated image (YOLO's `plot()` method adds bounding boxes)\n",
    "    annotated_frame = result.plot()\n",
    "\n",
    "    # Display the image with detections\n",
    "    cv2.imshow(\"YOLOv8 Detection\", annotated_frame)\n",
    "    cv2.waitKey(0)  # Wait for a key press\n",
    "    cv2.destroyAllWindows()  # Close the display window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78e9a6-b43b-43cd-b53a-f30c913757d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import os\n",
    "\n",
    "# Load Haar cascade classifiers for face, eyes, and smile detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\n",
    "\n",
    "# Text-to-speech engine\n",
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "stable_eye_count = {}\n",
    "stable_smile_count = {}\n",
    "STABLE_THRESHOLD = 10  # Require detections over multiple frames\n",
    "\n",
    "# Get the person's name\n",
    "person_name = input(\"Enter your name: \")\n",
    "base_folder = f\"dataset/{person_name}\"\n",
    "os.makedirs(base_folder, exist_ok=True)  # Create a main folder for the person\n",
    "\n",
    "def detect_faces(frame):\n",
    "    global stable_eye_count, stable_smile_count\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "    \n",
    "    detected_faces = 0\n",
    "    smiling_faces = 0\n",
    "    face_regions = []\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Detect eyes and smiles\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=6)\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.5, minNeighbors=25)\n",
    "        \n",
    "        face_id = f\"face_{detected_faces}\"\n",
    "        detected_faces += 1\n",
    "        \n",
    "        # Initialize counts if not present\n",
    "        if face_id not in stable_eye_count:\n",
    "            stable_eye_count[face_id] = 0\n",
    "        if face_id not in stable_smile_count:\n",
    "            stable_smile_count[face_id] = 0\n",
    "        \n",
    "        # Update counts based on detection stability\n",
    "        stable_eye_count[face_id] = max(0, stable_eye_count[face_id] - 1) if len(eyes) < 2 else stable_eye_count[face_id] + 1\n",
    "        stable_smile_count[face_id] = max(0, stable_smile_count[face_id] - 1) if len(smiles) == 0 else stable_smile_count[face_id] + 1\n",
    "        \n",
    "        print(f\"Person {detected_faces} - Eyes: {stable_eye_count[face_id]}, Smile: {stable_smile_count[face_id]}\")\n",
    "        \n",
    "        # Determine rectangle color\n",
    "        face_color = (0, 255, 0) if stable_eye_count[face_id] >= STABLE_THRESHOLD and stable_smile_count[face_id] >= STABLE_THRESHOLD else (0, 0, 255)\n",
    "        \n",
    "        # Draw face rectangle\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), face_color, 2)\n",
    "        \n",
    "        # Display person's name above the rectangle if green\n",
    "        if face_color == (0, 255, 0):\n",
    "            cv2.putText(frame, person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # If the face is stable, store its region\n",
    "        if face_color == (0, 255, 0):\n",
    "            smiling_faces += 1\n",
    "            face_regions.append((x, y, w, h))\n",
    "    \n",
    "    return frame, detected_faces, smiling_faces, face_regions\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)  # Increase FPS for smoother capture\n",
    "    \n",
    "    pic_count = 0\n",
    "    max_pics = 10\n",
    "    \n",
    "    while pic_count < max_pics:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame, detected_faces, smiling_faces, face_regions = detect_faces(frame)\n",
    "        \n",
    "        cv2.imshow('Face Detection', frame)\n",
    "        \n",
    "        # Capture only if all detected faces are smiling (green rectangle)\n",
    "        if detected_faces > 0 and detected_faces == smiling_faces:\n",
    "            for (x, y, w, h) in face_regions:\n",
    "                face_crop = frame[y:y+h, x:x+w]  # Crop only the detected face\n",
    "                filename = f\"{base_folder}/{person_name}_{pic_count}.jpg\"\n",
    "                cv2.imwrite(filename, face_crop)\n",
    "                print(f\"Picture captured: {filename}\")\n",
    "                pic_count += 1\n",
    "                \n",
    "                if pic_count >= max_pics:\n",
    "                    break  # Stop once 500 images are captured\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:  # Press 'q' or ESC to exit\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    speak(\"All 10 pictures have been taken successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ee16e-13eb-4050-a79a-02b2d92a51c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
