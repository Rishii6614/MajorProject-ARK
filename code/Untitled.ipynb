{
 "cells": [
  {
   "cell_type": "raw",
   "id": "32f1c265-e61e-4b4e-9b24-871464d4a8ca",
   "metadata": {},
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ca54160-00f2-44a4-85d7-f78d73bf8460",
   "metadata": {},
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9ee74dc-7e7e-4946-849d-e3c19a4de11e",
   "metadata": {},
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb315f-aec2-4f55-bdfd-6abdb8d89dd1",
   "metadata": {},
   "source": [
    "## smile detection and taking pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdb55b73-7b3b-4e8a-bb48-3c11d0ca085d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 1\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 1\n",
      "Person 1 - Eyes: 0, Smile: 2\n",
      "Person 1 - Eyes: 0, Smile: 1\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 1\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 1\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 2 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 1\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 1\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 3, Smile: 0\n",
      "Person 1 - Eyes: 4, Smile: 0\n",
      "Person 1 - Eyes: 3, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 3, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 3, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 2 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 0, Smile: 0\n",
      "Person 1 - Eyes: 1, Smile: 0\n",
      "Person 1 - Eyes: 2, Smile: 0\n",
      "Person 1 - Eyes: 3, Smile: 0\n",
      "Person 1 - Eyes: 4, Smile: 0\n",
      "Person 1 - Eyes: 5, Smile: 0\n",
      "Person 1 - Eyes: 6, Smile: 0\n",
      "Person 1 - Eyes: 7, Smile: 0\n",
      "Person 1 - Eyes: 8, Smile: 0\n",
      "Person 1 - Eyes: 9, Smile: 0\n",
      "Person 1 - Eyes: 10, Smile: 0\n",
      "Person 1 - Eyes: 11, Smile: 0\n",
      "Person 1 - Eyes: 12, Smile: 0\n",
      "Person 1 - Eyes: 13, Smile: 0\n",
      "Person 1 - Eyes: 14, Smile: 0\n",
      "Person 1 - Eyes: 15, Smile: 0\n",
      "Person 1 - Eyes: 16, Smile: 0\n",
      "Person 1 - Eyes: 17, Smile: 1\n",
      "Person 1 - Eyes: 18, Smile: 2\n",
      "Person 1 - Eyes: 19, Smile: 3\n",
      "Person 1 - Eyes: 20, Smile: 4\n",
      "Person 1 - Eyes: 21, Smile: 5\n",
      "Person 1 - Eyes: 22, Smile: 6\n",
      "Person 1 - Eyes: 23, Smile: 7\n",
      "Person 1 - Eyes: 24, Smile: 8\n",
      "Person 1 - Eyes: 25, Smile: 9\n",
      "Person 1 - Eyes: 26, Smile: 10\n",
      "Console Output: Picture captured and saved as captured_image_1.jpg!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import pyttsx3\n",
    "\n",
    "# Load Haar cascade classifiers for face, eyes, and smile detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "stable_eye_count = {}\n",
    "stable_smile_count = {}\n",
    "STABLE_THRESHOLD = 10  # Require detections over multiple frames\n",
    "\n",
    "def detect_faces(frame):\n",
    "    global stable_eye_count, stable_smile_count\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "    \n",
    "    detected_faces = 0\n",
    "    smiling_faces = 0\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Detect eyes and smiles\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=6)\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.5, minNeighbors=25)\n",
    "        \n",
    "        face_id = f\"face_{detected_faces}\"\n",
    "        detected_faces += 1\n",
    "        \n",
    "        # Initialize counts if not present\n",
    "        if face_id not in stable_eye_count:\n",
    "            stable_eye_count[face_id] = 0\n",
    "        if face_id not in stable_smile_count:\n",
    "            stable_smile_count[face_id] = 0\n",
    "        \n",
    "        # Update counts based on detection stability\n",
    "        stable_eye_count[face_id] = max(0, stable_eye_count[face_id] - 1) if len(eyes) < 2 else stable_eye_count[face_id] + 1\n",
    "        stable_smile_count[face_id] = max(0, stable_smile_count[face_id] - 1) if len(smiles) == 0 else stable_smile_count[face_id] + 1\n",
    "        \n",
    "        print(f\"Person {detected_faces} - Eyes: {stable_eye_count[face_id]}, Smile: {stable_smile_count[face_id]}\")\n",
    "        \n",
    "        # Determine colors for rectangles\n",
    "        face_color = (0, 255, 0) if stable_eye_count[face_id] >= STABLE_THRESHOLD and stable_smile_count[face_id] >= STABLE_THRESHOLD else (0, 0, 255)\n",
    "        smile_color = (0, 255, 0) if stable_smile_count[face_id] >= STABLE_THRESHOLD else (0, 0, 255)\n",
    "        \n",
    "        # Draw face rectangle\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), face_color, 2)  \n",
    "        \n",
    "        # Draw smile rectangle if detected\n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            cv2.rectangle(roi_color, (sx, sy), (sx+sw, sy+sh), smile_color, 2)  # Rectangle over the mouth\n",
    "        \n",
    "        if stable_eye_count[face_id] >= STABLE_THRESHOLD and stable_smile_count[face_id] >= STABLE_THRESHOLD:\n",
    "            smiling_faces += 1\n",
    "    \n",
    "    return frame, detected_faces, smiling_faces\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)  # Increase FPS for smoother capture\n",
    "    \n",
    "    pic_count = 1\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Make a copy of the original frame before drawing rectangles\n",
    "        clean_frame = frame.copy()\n",
    "        \n",
    "        frame, detected_faces, smiling_faces = detect_faces(frame)\n",
    "        \n",
    "        cv2.imshow('Face Detection', frame)\n",
    "        \n",
    "        # Check if all detected faces are smiling\n",
    "        if detected_faces > 0 and detected_faces == smiling_faces:  \n",
    "            speak(\"3, 2, 1, Cheese!\")\n",
    "            \n",
    "            # Save the clean frame (without rectangles)\n",
    "            filename = f\"captured_image_{pic_count}.jpg\"\n",
    "            cv2.imwrite(filename, clean_frame)\n",
    "            \n",
    "            print(f\"Console Output: Picture captured and saved as {filename}!\")\n",
    "            speak(\"Picture taken!\")\n",
    "            break  # Exit after taking the picture\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:  # Press 'q' or ESC to exit\n",
    "            print(\"Console Output: Exiting...\")\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f82ae60b-92e0-4735-966f-d1499cc80fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "\n",
    "# Load pre-trained face detection model  \n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')  \n",
    "\n",
    "# Open webcam  \n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "while True:  \n",
    "    ret, frame = cap.read()  \n",
    "    if not ret:  \n",
    "        break  \n",
    "\n",
    "    # Convert to grayscale for better accuracy  \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  \n",
    "\n",
    "    # Detect faces  \n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))  \n",
    "\n",
    "    for (x, y, w, h) in faces:  \n",
    "        # Draw bounding box around the face  \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)  \n",
    "\n",
    "        # Add label  \n",
    "        cv2.putText(frame, \"Face Detected\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)  \n",
    "\n",
    "    # Show output  \n",
    "    cv2.imshow(\"Face Detection\", frame)  \n",
    "\n",
    "    # Press 'q' to exit  \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  \n",
    "        break  \n",
    "\n",
    "cap.release()  \n",
    "cv2.destroyAllWindows()  \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09a338c8-2c5c-406d-8e1f-d157fae16f7e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8d7b3f7-4e3e-4980-b6da-cb1a9f39c874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 181.4ms\n",
      "Speed: 6.7ms preprocess, 181.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load YOLO model (Ensure you have the correct YOLOv8 model)\n",
    "model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "# Load image\n",
    "image_path = \"captured_image_1.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Run YOLO detection\n",
    "results = model(image)\n",
    "\n",
    "# Process results and display output\n",
    "for result in results:\n",
    "    # Get annotated image (YOLO's `plot()` method adds bounding boxes)\n",
    "    annotated_frame = result.plot()\n",
    "\n",
    "    # Display the image with detections\n",
    "    cv2.imshow(\"YOLOv8 Detection\", annotated_frame)\n",
    "    cv2.waitKey(0)  # Wait for a key press\n",
    "    cv2.destroyAllWindows()  # Close the display window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c5370-0bd0-433e-af95-192180357908",
   "metadata": {},
   "source": [
    "### Saving images of the person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4ee16e-13eb-4050-a79a-02b2d92a51c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  Kavish.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1 - Eyes: 0\n",
      "Person 1 - Eyes: 0\n",
      "Person 1 - Eyes: 1\n",
      "Person 1 - Eyes: 2\n",
      "Person 1 - Eyes: 3\n",
      "Person 1 - Eyes: 4\n",
      "Person 1 - Eyes: 5\n",
      "Person 1 - Eyes: 6\n",
      "Person 1 - Eyes: 7\n",
      "Person 1 - Eyes: 8\n",
      "Person 1 - Eyes: 7\n",
      "Person 1 - Eyes: 8\n",
      "Person 1 - Eyes: 9\n",
      "Person 1 - Eyes: 10\n",
      "Picture captured: dataset/Kavish./Kavish._0.jpg\n",
      "Person 1 - Eyes: 11\n",
      "Picture captured: dataset/Kavish./Kavish._1.jpg\n",
      "Person 1 - Eyes: 12\n",
      "Picture captured: dataset/Kavish./Kavish._2.jpg\n",
      "Person 1 - Eyes: 13\n",
      "Picture captured: dataset/Kavish./Kavish._3.jpg\n",
      "Person 1 - Eyes: 14\n",
      "Picture captured: dataset/Kavish./Kavish._4.jpg\n",
      "Person 1 - Eyes: 15\n",
      "Picture captured: dataset/Kavish./Kavish._5.jpg\n",
      "Person 1 - Eyes: 16\n",
      "Picture captured: dataset/Kavish./Kavish._6.jpg\n",
      "Person 1 - Eyes: 17\n",
      "Picture captured: dataset/Kavish./Kavish._7.jpg\n",
      "Person 1 - Eyes: 16\n",
      "Picture captured: dataset/Kavish./Kavish._8.jpg\n",
      "Person 1 - Eyes: 17\n",
      "Picture captured: dataset/Kavish./Kavish._9.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import os\n",
    "\n",
    "# Load Haar cascade classifiers for face and eyes detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Text-to-speech engine\n",
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "stable_eye_count = {}\n",
    "STABLE_THRESHOLD = 10  # Require detections over multiple frames\n",
    "\n",
    "# Get the person's name\n",
    "person_name = input(\"Enter your name: \")\n",
    "base_folder = f\"dataset/{person_name}\"\n",
    "os.makedirs(base_folder, exist_ok=True)  # Create a main folder for the person\n",
    "\n",
    "def detect_faces(frame):\n",
    "    global stable_eye_count\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "    \n",
    "    detected_faces = 0\n",
    "    face_regions = []\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=6)\n",
    "        \n",
    "        face_id = f\"face_{detected_faces}\"\n",
    "        detected_faces += 1\n",
    "        \n",
    "        # Initialize counts if not present\n",
    "        if face_id not in stable_eye_count:\n",
    "            stable_eye_count[face_id] = 0\n",
    "        \n",
    "        # Update counts based on detection stability\n",
    "        stable_eye_count[face_id] = max(0, stable_eye_count[face_id] - 1) if len(eyes) < 2 else stable_eye_count[face_id] + 1\n",
    "        \n",
    "        print(f\"Person {detected_faces} - Eyes: {stable_eye_count[face_id]}\")\n",
    "        \n",
    "        # Determine rectangle color\n",
    "        face_color = (0, 255, 0) if stable_eye_count[face_id] >= STABLE_THRESHOLD else (0, 0, 255)\n",
    "        \n",
    "        # Draw face rectangle\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), face_color, 2)\n",
    "        \n",
    "        # Display person's name above the rectangle if green\n",
    "        if face_color == (0, 255, 0):\n",
    "            cv2.putText(frame, person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            face_regions.append((x, y, w, h))\n",
    "    \n",
    "    return frame, detected_faces, face_regions\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)  # Increase FPS for smoother capture\n",
    "    \n",
    "    pic_count = 0\n",
    "    max_pics = 10\n",
    "    \n",
    "    while pic_count < max_pics:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame, detected_faces, face_regions = detect_faces(frame)\n",
    "        \n",
    "        cv2.imshow('Face Detection', frame)\n",
    "        \n",
    "        # Capture only if detected faces are within the green rectangle\n",
    "        if detected_faces > 0:\n",
    "            for (x, y, w, h) in face_regions:\n",
    "                face_crop = frame[y:y+h, x:x+w]  # Crop only the detected face\n",
    "                filename = f\"{base_folder}/{person_name}_{pic_count}.jpg\"\n",
    "                cv2.imwrite(filename, face_crop)\n",
    "                print(f\"Picture captured: {filename}\")\n",
    "                pic_count += 1\n",
    "                \n",
    "                if pic_count >= max_pics:\n",
    "                    break  # Stop once 10 images are captured\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:  # Press 'q' or ESC to exit\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    speak(\"All 10 pictures have been taken successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db32231d-799f-40aa-85de-1353d7b5b47b",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "##labels = []\n",
    "\n",
    "# Ensure labels is a 1D array\n",
    "labels = np.array(labels).flatten()  # Convert to 1D if needed\n",
    "\n",
    "# Encode labels (convert names into numerical categories)\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)  # Convert names to numbers\n",
    "num_classes = len(np.unique(labels_encoded))  # Get unique class count\n",
    "\n",
    "# Convert labels to categorical (for softmax output)\n",
    "labels_categorical = to_categorical(labels_encoded, num_classes)\n",
    "\n",
    "# Print to check shapes\n",
    "print(f\"Labels Encoded Shape: {labels_encoded.shape}\")\n",
    "print(f\"Labels Categorical Shape: {labels_categorical.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee59c7d-8a06-41ab-a043-1c85a4d63522",
   "metadata": {},
   "source": [
    "### Face detecting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1da1af43-1fe4-44d7-87d7-bee023b67882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 5s 591ms/step - loss: 1.7554 - accuracy: 0.3853 - val_loss: 1.5224 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 542ms/step - loss: 1.5604 - accuracy: 0.5413 - val_loss: 1.4880 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 579ms/step - loss: 1.5056 - accuracy: 0.5413 - val_loss: 1.4484 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 451ms/step - loss: 1.5272 - accuracy: 0.5413 - val_loss: 1.3896 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 481ms/step - loss: 1.4385 - accuracy: 0.5413 - val_loss: 1.3871 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 556ms/step - loss: 1.3807 - accuracy: 0.5413 - val_loss: 1.3599 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 2s 482ms/step - loss: 1.3243 - accuracy: 0.5413 - val_loss: 1.2174 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 2s 555ms/step - loss: 1.2181 - accuracy: 0.5688 - val_loss: 1.1211 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 2s 580ms/step - loss: 1.1014 - accuracy: 0.5872 - val_loss: 0.8511 - val_accuracy: 0.6429\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 2s 486ms/step - loss: 0.9130 - accuracy: 0.6606 - val_loss: 0.6658 - val_accuracy: 0.6786\n",
      "Model training completed and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22ag5\\anaconda3\\envs\\financial\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ===================== Load & Preprocess Data =====================\n",
    "\n",
    "# Path to dataset\n",
    "dataset_path = \"dataset\"  # Update this path\n",
    "\n",
    "# Define image size\n",
    "img_size = 100  # Resize images to 100x100\n",
    "\n",
    "# Lists to store images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load images and labels\n",
    "for person_name in os.listdir(dataset_path):\n",
    "    person_path = os.path.join(dataset_path, person_name)\n",
    "    \n",
    "    if os.path.isdir(person_path):  # Ensure it's a folder\n",
    "        for img_name in os.listdir(person_path):\n",
    "            img_path = os.path.join(person_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, (img_size, img_size))  # Resize image\n",
    "                images.append(img)\n",
    "                labels.append(person_name)  # Use folder name as label\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "images = np.array(images) / 255.0  # Normalize pixel values\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(np.unique(labels_encoded))\n",
    "\n",
    "# Convert labels to categorical (for softmax output)\n",
    "labels_categorical = to_categorical(labels_encoded, num_classes)\n",
    "\n",
    "# Ensure consistency in shape\n",
    "assert images.shape[0] == labels_categorical.shape[0], \"Mismatch between images and labels\"\n",
    "\n",
    "# ===================== Train-Test Split =====================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# ===================== Build CNN Model =====================\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ===================== Train Model =====================\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save trained model\n",
    "model.save(\"face_recognition_model.h5\")\n",
    "np.save(\"label_classes.npy\", label_encoder.classes_)  # Save label classes\n",
    "\n",
    "print(\"Model training completed and saved!\")\n",
    "\n",
    "# ===================== Real-Time Face Detection =====================\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"face_recognition_model.h5\")\n",
    "label_encoder.classes_ = np.load(\"label_classes.npy\", allow_pickle=True)\n",
    "\n",
    "# Load OpenCV face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set full-screen mode\n",
    "cv2.namedWindow(\"Face Recognition\", cv2.WND_PROP_FULLSCREEN)\n",
    "cv2.setWindowProperty(\"Face Recognition\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]  # Extract face\n",
    "        face = cv2.resize(face, (img_size, img_size)) / 255.0  # Preprocess\n",
    "        face = np.expand_dims(face, axis=0)  # Reshape for CNN\n",
    "        \n",
    "        prediction = model.predict(face)\n",
    "        class_index = np.argmax(prediction)\n",
    "        person_name = label_encoder.inverse_transform([class_index])[0]  # Decode label\n",
    "        \n",
    "        # Draw rectangle & label\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, person_name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44827056-07ad-47dc-a46d-c8eec5fb310e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf00d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
